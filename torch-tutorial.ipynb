{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef99f8f6-57b2-49c5-9ee2-4efe8d44da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import struct\n",
    "import gzip\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe7bfa0-eaab-4ae2-ba3c-e5b00d30a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todos\n",
    "# explain f1\n",
    "# torch example (dataset, model, gpu, training loop); (conv)\n",
    "# explain relations\n",
    "# explain autograd and dag; similarity to sgd [make sgd example]\n",
    "# show adam optim\n",
    "# relu and activation functions\n",
    "# batch norm\n",
    "# monitor val loss (see sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5509bd1-973d-4df2-bfb5-2aa96554120b",
   "metadata": {},
   "source": [
    "### Downloading MNIST Dataset from Offical Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d56f38a-8c8b-44f3-a34f-c4c28576c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./data'):\n",
    "    os.makedirs('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d279cd90-8e08-43b6-9de1-2a79f7e26c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "    'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "    'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "    'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
    "]\n",
    "for url in urls:\n",
    "    r = requests.get(url)\n",
    "    with open(f'./data/{url.split(\"/\")[-1]}', 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbbd66b-e270-4e6c-9424-02c110aafbfb",
   "metadata": {},
   "source": [
    "The following structure below represents the training set `train-images-idx3-ubyte.gz`\n",
    "\n",
    "```r\n",
    "[offset] [type]          [value]          [description]\n",
    "0000     32 bit integer  0x00000803(2051) magic number\n",
    "0004     32 bit integer  60000            number of images\n",
    "0008     32 bit integer  28               number of rows\n",
    "0012     32 bit integer  28               number of columns\n",
    "0016     unsigned byte   ??               pixel\n",
    "0017     unsigned byte   ??               pixel\n",
    "........\n",
    "xxxx     unsigned byte   ??               pixel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652aba3f-890e-4831-87d9-64c26118321b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading MNIST data (crash course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33194952-6451-4322-8ad8-7593b3e63609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x00\\x00\\x08\\x03'\n",
      "b'\\x00\\x00\\xea`'\n",
      "b'\\x00\\x00\\x00\\x1c'\n",
      "b'\\x00\\x00\\x00\\x1c'\n",
      "b'\\x00\\x00\\x00\\x00'\n",
      "b'\\x00\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "# sample content\n",
    "with gzip.open('./data/train-images-idx3-ubyte.gz','rb') as f:\n",
    "    print(f.read(4)) # 2051 (magic number\n",
    "    print(f.read(4)) # 60000 (n samples) (backtick == b'\\x60')\n",
    "    print(f.read(4)) # 28 (n rows)\n",
    "    print(f.read(4)) # 28 (n columns)\n",
    "    print(f.read(4)) # 0, 0, 0, 0 (pixel data points)\n",
    "    print(f.read(4)) # 0, 0, 0, 0 (pixel data points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a66bd-0fff-4f4c-a143-c02f03a0b9cb",
   "metadata": {},
   "source": [
    "8 bits in 1 byte (value ranges from 0-255).  \n",
    "Most people use hexadecimal (base 16) to represent bytes since it's more compact and divides evenly.  \n",
    "The values 0-255 in hexdecimal is 0x00 - 0xFF.  \n",
    "So each byte (== 8 bits) is represented by two hexadecimals digits.\n",
    "\n",
    "Basically, the data starts on the 16th byte and everything before that is metadata.  \n",
    "We show below how to read this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c54b466-9fe5-4c51-8607-c907c13cd4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39969045/parsing-yann-lecuns-mnist-idx-file-format\n",
    "\n",
    "with gzip.open('./data/train-images-idx3-ubyte.gz','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    X_train = np.frombuffer(f.read(), dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    X_train = X_train.reshape((size, nrows, ncols))\n",
    "\n",
    "with gzip.open('./data/train-labels-idx1-ubyte.gz','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    y_train = np.frombuffer(f.read(), dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "\n",
    "with gzip.open('./data/t10k-images-idx3-ubyte.gz','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    X_test = np.frombuffer(f.read(), dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    X_test = X_test.reshape((size, nrows, ncols))\n",
    "\n",
    "with gzip.open('./data/t10k-labels-idx1-ubyte.gz','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    y_test = np.frombuffer(f.read(), dtype=np.dtype(np.uint8).newbyteorder('>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201769cb-ec26-4e80-a4fb-1b20fa857588",
   "metadata": {},
   "source": [
    "### Explaining the Code (ChatGPT)\n",
    "\n",
    "In the given code snippet, the `struct.unpack` function is used to extract data from a binary file according to a specified format. The `struct` module in Python provides functions for working with C-style data structures represented as strings. It allows you to pack and unpack data in a binary format.\n",
    "\n",
    "Let's break down the usage of `struct.unpack` in the code:\n",
    "\n",
    "1. `magic, size = struct.unpack(\">II\", f.read(8))`\n",
    "   Here, `f.read(8)` reads 8 bytes from the file object `f`. The format string `\">II\"` specifies the format of the data to be unpacked. `\">\"` indicates big-endian byte order, and `I` represents an unsigned integer of size 4 bytes. Therefore, `struct.unpack(\">II\", f.read(8))` reads 8 bytes from the file, interprets the first 4 bytes as `magic` and the next 4 bytes as `size`, and assigns the unpacked values to the variables `magic` and `size`.\n",
    "\n",
    "2. `nrows, ncols = struct.unpack(\">II\", f.read(8))`\n",
    "   This line is similar to the previous one. It reads another 8 bytes from the file and interprets the first 4 bytes as `nrows` and the next 4 bytes as `ncols`.\n",
    "\n",
    "3. `data = np.frombuffer(f.read(), dtype=np.dtype(np.uint8).newbyteorder('>'))`\n",
    "   Here, `f.read()` reads the remaining data from the file. The `frombuffer` function from the NumPy library is used to create an array from the binary data. The data type is specified as `np.uint8`, representing an 8-bit unsigned integer. The `newbyteorder('>')` method is used to ensure the data is interpreted in big-endian byte order.\n",
    "\n",
    "4. `data = data.reshape((size, nrows, ncols))`\n",
    "   Finally, the `data` array is reshaped using the dimensions obtained from the previous unpacking steps. It is reshaped into a 3-dimensional array with `size` rows, `nrows` height, and `ncols` width."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344ce33-2a76-4938-8aa5-caa2b848e760",
   "metadata": {},
   "source": [
    "### Explaining Big vs Little Endian (ChatGPT)\n",
    "\n",
    "There are two common types of endianness: big-endian and little-endian.\n",
    "\n",
    "Big-endian: In big-endian systems, the most significant byte (the byte containing the highest order bits) is stored at the lowest memory address, while the least significant byte is stored at the highest memory address. This means that the data is stored from left to right, with the most significant byte first.\n",
    "\n",
    "Little-endian: In little-endian systems, the least significant byte is stored at the lowest memory address, while the most significant byte is stored at the highest memory address. The data is stored from right to left, with the least significant byte first.\n",
    "\n",
    "To understand this concept, let's consider a 4-byte integer value 0x12345678. Here's how it would be stored in memory based on the endianness:\n",
    "\n",
    "Big-endian:\n",
    "\n",
    "Memory Address: 0x00 0x01 0x02 0x03\n",
    "Data Value: 0x12 0x34 0x56 0x78\n",
    "Little-endian:\n",
    "\n",
    "Memory Address: 0x00 0x01 0x02 0x03\n",
    "Data Value: 0x78 0x56 0x34 0x12\n",
    "\n",
    "Note that the endianness affects the ordering of bytes, but not the individual bits within each byte.\n",
    "\n",
    "Endianness is important when data is shared between systems or when reading data from a binary file format. It's essential to ensure that both the sender and receiver of data interpret the bytes in the correct order to avoid data corruption or misinterpretation. Most modern systems, including x86 and ARM processors, use little-endian architecture. However, big-endian systems are still in use, particularly in certain network protocols or older hardware architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6160c4ac-d2fd-4001-890d-cf3dfbedc3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4b9597-67ef-432d-87e4-cd28b8dc6cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15107dcf010>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48d98cdb-70c2-4b08-98ae-c696a6a9d393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7575508c-c342-49b3-835d-664c90a55d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(len(X_train), -1)\n",
    "X_test_flattened = X_test.reshape(len(X_test), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21a4e0d7-1625-4c0a-8fd1-3defb9f0ab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_flattened.shape)\n",
    "print(X_test_flattened.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32787e-341b-489e-a2c4-463ddfe61379",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b3145ac-1ac1-4422-876d-c5ea8ff2722f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 16s\n",
      "Wall time: 2min 16s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log_loss')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "classifier_sgd = SGDClassifier(\n",
    "    loss='log_loss',\n",
    "    penalty='l2',\n",
    "    alpha=0.0001,\n",
    "    l1_ratio=0.15,\n",
    "    fit_intercept=True,\n",
    "    max_iter=1000,\n",
    "    tol=0.001,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    epsilon=0.1,\n",
    "    n_jobs=None,\n",
    "    random_state=None,\n",
    "    learning_rate='optimal',\n",
    "    eta0=0.0,\n",
    "    power_t=0.5,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=5,\n",
    "    class_weight=None,\n",
    "    warm_start=False,\n",
    "    average=False,\n",
    ")\n",
    "classifier_sgd.fit(X_train_flattened, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd35e389-06ee-4f2d-bb6b-4369dd1ab06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95      1037\n",
      "           1       0.95      0.98      0.97      1100\n",
      "           2       0.80      0.94      0.87       879\n",
      "           3       0.81      0.95      0.87       859\n",
      "           4       0.90      0.90      0.90       975\n",
      "           5       0.86      0.85      0.85       902\n",
      "           6       0.93      0.94      0.94       948\n",
      "           7       0.81      0.98      0.89       852\n",
      "           8       0.94      0.65      0.77      1413\n",
      "           9       0.85      0.83      0.84      1035\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.89      0.88     10000\n",
      "weighted avg       0.89      0.88      0.88     10000\n",
      "\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 34.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = classifier_sgd.predict(X_test_flattened)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a23388c-dcec-44f6-b6ee-035f9cb6d6ad",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dab6e6d4-41c5-48d6-8fdd-f5f5f63c814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 21min\n",
      "Wall time: 2min 39s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(128,), solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(128,), solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=64, hidden_layer_sizes=(128,), solver='sgd')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "classifier_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128,),\n",
    "    activation='relu',\n",
    "    solver='sgd',\n",
    "    alpha=0.0001,\n",
    "    batch_size=64,\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=1e-3,\n",
    "    power_t=0.5,\n",
    "    max_iter=200,\n",
    "    shuffle=True,\n",
    "    random_state=None,\n",
    "    tol=0.0001,\n",
    "    verbose=False,\n",
    "    warm_start=False,\n",
    "    momentum=0.9,\n",
    "    nesterovs_momentum=True,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    n_iter_no_change=10,\n",
    "    max_fun=15000,\n",
    ")\n",
    "classifier_mlp.fit(X_train_flattened, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c8c284a-25d9-49d3-9a6c-63d5b1ef6732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       975\n",
      "           1       0.99      0.97      0.98      1154\n",
      "           2       0.85      0.97      0.91       902\n",
      "           3       0.90      0.95      0.93       962\n",
      "           4       0.90      0.97      0.93       908\n",
      "           5       0.87      0.98      0.92       791\n",
      "           6       0.97      0.95      0.96       978\n",
      "           7       0.93      0.97      0.95       989\n",
      "           8       0.96      0.71      0.81      1314\n",
      "           9       0.93      0.91      0.92      1027\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.94      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n",
      "CPU times: total: 375 ms\n",
      "Wall time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = classifier_mlp.predict(X_test_flattened)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6df15-a6d8-4a92-97b2-a575eac214a5",
   "metadata": {},
   "source": [
    "### MLP VS SGD (ChatGPT)\n",
    "\n",
    "Algorithm Family:\n",
    "\n",
    "MLPClassifier: It is based on artificial neural networks, specifically multilayer perceptron (MLP) networks. MLP networks consist of multiple layers of nodes, including input, hidden, and output layers. They are known as universal function approximators and can handle complex non-linear relationships in data.\n",
    "SGDClassifier: It is based on the stochastic gradient descent (SGD) optimization algorithm. SGD is a widely used optimization algorithm for training various types of models, including linear classifiers. It iteratively updates the model parameters using a small subset of the training data (mini-batches) to approximate the gradient of the loss function.\n",
    "Flexibility:\n",
    "\n",
    "MLPClassifier: It can model complex non-linear relationships and is capable of learning high-dimensional representations. It can handle a wide range of data types and can capture intricate patterns and interactions.\n",
    "SGDClassifier: It is a linear classifier and is generally better suited for simpler, linearly separable problems. It is less flexible in terms of modeling complex relationships and may not perform as well on highly non-linear datasets.\n",
    "Training:\n",
    "\n",
    "MLPClassifier: Training an MLP model can be computationally expensive, especially for large datasets and complex architectures. It often requires more data and longer training times to achieve optimal performance. It may also be prone to overfitting if not properly regularized.\n",
    "SGDClassifier: Training an SGD model is generally faster and more efficient, especially for large-scale datasets. It is computationally less demanding and can handle large amounts of data efficiently. However, it may require careful hyperparameter tuning to achieve optimal performance.\n",
    "Interpretability:\n",
    "\n",
    "MLPClassifier: MLP models are often considered as black boxes, meaning they can be challenging to interpret. The hidden layers and the complex interactions between nodes make it difficult to understand the internal workings and the importance of individual features.\n",
    "SGDClassifier: SGD models, being linear classifiers, offer greater interpretability. The model coefficients directly represent the feature importance and can provide insights into the relationships between features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bb25e-6899-4c1c-918a-9c65f497f2ba",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b39b67a5-08ad-4c09-b2f5-5d46c340d5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 39min 18s\n",
      "Wall time: 2min 51s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "classifier_xgb = XGBClassifier() # too many params to list lol\n",
    "classifier_xgb.fit(X_train_flattened, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3525f69b-94de-4318-928e-27e34560a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       993\n",
      "           1       0.99      0.99      0.99      1136\n",
      "           2       0.98      0.98      0.98      1033\n",
      "           3       0.98      0.98      0.98      1013\n",
      "           4       0.98      0.98      0.98       973\n",
      "           5       0.98      0.98      0.98       886\n",
      "           6       0.98      0.98      0.98       955\n",
      "           7       0.97      0.98      0.98      1023\n",
      "           8       0.98      0.98      0.98       972\n",
      "           9       0.97      0.97      0.97      1016\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "CPU times: total: 953 ms\n",
      "Wall time: 86.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = classifier_xgb.predict(X_test_flattened)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1efd34b-efd4-4822-ba5d-e0ef033720c5",
   "metadata": {},
   "source": [
    "### Why Better?\n",
    "\n",
    "- data is nonlinear  \n",
    "- logr can only model linear relationship between Y ~ X  \n",
    "- xgb is an ensemble method that can model nonlinearities and interactions between features  \n",
    "- decision trees also have some inherent robustness against by outliers  \n",
    "- ensemble learning also reduce both bias and var of a model  \n",
    "\n",
    "to improve logr performance, one would need careful feature engineering including:\n",
    "- nonlinear transformations\n",
    "- finding interactions between features\n",
    "- handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84661c52-22a6-451f-90dc-1f8107cd501e",
   "metadata": {},
   "source": [
    "### Refresher / Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39780246-99ba-4b76-95f4-4fb8a0b1341f",
   "metadata": {},
   "source": [
    "Machine Learning can broadly fall into these categories:\n",
    "\n",
    "- Unsupervised $(X, ?)$\n",
    "- Supervised $(X, y)$\n",
    "    \n",
    "    Say we have want to create a predictive model $f$ s.t. $\\hat{y}_\\theta = f(X, \\theta)$  \n",
    "    We define the loss function to be some distance function $D$ between the true value $y$ and predicted value $\\hat{y}$ s.t. $L(\\theta) = D(y, \\hat{y}_\\theta)$  \n",
    "    In order to \"fit\" or \"optimize\" the model, our goal is to minimize the loss.  \n",
    "    Different supervised learning problems uses different loss functions.  \n",
    "\n",
    "    - Regression (y is continuous) (losses: squared error, abosolute error, huber)\n",
    "    - Classification  (y is discrete) (losses: cross-entropy, log, hinge)\n",
    "        - multi-label\n",
    "        - single-label\n",
    "            - binary\n",
    "            - multi-class <- our MNIST problem falls here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab964dc4-5026-4518-bbc8-f8cdb5ea134b",
   "metadata": {},
   "source": [
    "Many of our supervised learning problems is trying to close the discrepancy between predicted and actual outcomes, which can be simplified as the minimization of a loss function.\n",
    "\n",
    "Some machine learning algorithms can directly solve for a closed solution for the model weights that minimizes the loss (i.e., linear/logistic regression).\n",
    "\n",
    "However, when it comes to more complex models, a closed solution may not always be available. Neural networks, can be seen as a generalized version of linear/logistic regression where there are many nonlinearities. Thus we employ a different technique known as gradient descent to solve for these.\n",
    "\n",
    "This is an optimization algorithm that iteratively:\n",
    "\n",
    "    - Finds the gradient of the loss function with respect to the weights\n",
    "    - Updates the weights in the opposite direction of the gradient\n",
    "    \n",
    "Mathematically, this is seen as:  \n",
    "$\\theta_{new} = \\theta_{old} - \\text{learning_rate} * \\nabla{f(\\theta_{old}})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f373b3-a982-414a-8ec6-533efdd1883c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neural Nets Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c0d04-3815-4585-8ebc-c30ab9ebf1d2",
   "metadata": {},
   "source": [
    "<img src='./perceptron.png' />\n",
    "\n",
    "<img src='./network.png' />\n",
    "\n",
    "<img src='./backprop.gif' />\n",
    "\n",
    "<img src=\"./autograd.png\" style=\"background-color: white;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565ee85-20b6-406d-8856-5f2ba194f933",
   "metadata": {},
   "source": [
    "### Recommended Watch\n",
    "\n",
    "3Blue1Brown\n",
    "1. [What is a Neural Network](https://youtu.be/aircAruvnKk)\n",
    "2. [Gradient Descent](https://youtu.be/IHZwWFHWa-w)\n",
    "3. [Backpropagation Overview](https://youtu.be/Ilg3gGewQ5U)\n",
    "4. [Backpropagation Calculus](https://youtu.be/tIeHLnjs5U8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148fdc8-f4d0-415d-bdee-e9fd858ad0d9",
   "metadata": {},
   "source": [
    "### Pytorch Autograd (Crash Course)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890f8372-f60e-41a1-9039-4e0df072977c",
   "metadata": {},
   "source": [
    "Recommended Sources\n",
    "- https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "- https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "464a71be-c42c-4205-997b-36251db55864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array([[1, 2],[3, 4]])\n",
    "tensor = torch.tensor(array)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fc523-9326-4da7-bd87-20d617814001",
   "metadata": {},
   "source": [
    "Two major differences between the two:\n",
    "1. tensors stores automatically computed gradients between operations\n",
    "2. tensors leverage gpu support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7c1ec88-7828-4496-836f-b9ba1c0b2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True) # default is False\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9a34120-4780-4f7c-9ad9-6a3f023a0fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a60aec20-b6d4-4d71-9dc7-fa9d3d3b5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07181fc2-e590-479b-9527-612ce3b4f4da",
   "metadata": {},
   "source": [
    "$\\frac{\\delta Q}{\\delta a} = 9 a^2 = [36, 81]$\n",
    "\n",
    "$\\frac{\\delta Q}{\\delta b} = -2 b = [-12, -8]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88b800bb-15b5-4c6b-8789-24a9477fec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.]) # this value is multiplied element wise to final answer\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da5527f5-bdd9-4f48-b53a-c824fc4067ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36., 81.])\n",
      "tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad13907-d239-446b-bc78-b1af1c407bcb",
   "metadata": {},
   "source": [
    "### Hands On Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4b731eb-c3e4-4144-a145-509bc4858216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6855b35f-616c-464d-8d02-439eabca8fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset:\n",
    "    def __init__(self, X, y, add_channel_dimension=False):\n",
    "        assert X.shape[0] == len(y)\n",
    "        self.X = torch.tensor(X, dtype=torch.float32) # required dtype (too high precision will be slow)\n",
    "        self.y = torch.tensor(y, dtype=torch.int64) # required dtype\n",
    "\n",
    "        if add_channel_dimension: # for CNNs\n",
    "            self.add_channel_dimension()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "    def add_channel_dimension(self):\n",
    "        self.X = self.X.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0d9a402-c0aa-47d7-981f-4523f82a9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MNISTDataset(X_train, y_train)\n",
    "dataset_test = MNISTDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "209eec85-7f1a-4bf1-b86c-7bb98aa0cd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "X1, y1 = dataset_train[0]\n",
    "print(X1.shape)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d570f680-3614-45da-961c-467d0200ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDeepLearningModel:\n",
    "    def __init__(self, neural_net, loss_fn, optim, device=torch.device('cpu')):\n",
    "        self.neural_net = neural_net.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optim = optim\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, dataset, epochs=10, batch_size=64, validation_fraction=0.8):\n",
    "        dataset_train, dataset_val = random_split(dataset, lengths=[1-validation_fraction, validation_fraction])\n",
    "        # Fun Fact: `validation_fraction` also a parameter in scikit-learn SGDClassifier\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.neural_net.train() # Dropout and BatchNorm layers will behave differently\n",
    "            loss_train = 0\n",
    "            dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True) # dl aggregates small batches of data\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device) # move to device\n",
    "                y_pred = self.neural_net(X) # forward prop\n",
    "                loss = self.loss_fn(y_pred, y) # get error\n",
    "                loss.backward() # backprop\n",
    "                loss_train += self.loss_fn(y_pred, y).item()\n",
    "                self.optim.step() # update weights toward gradient where loss is min\n",
    "                self.optim.zero_grad() # zero the gradients for next iter\n",
    "            loss_train /= len(dataloader)\n",
    "\n",
    "            self.neural_net.eval() # Dropout and BatchNorm layers will behave differently\n",
    "            loss_val = 0\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size) # dl aggregates small batches of data\n",
    "            with torch.no_grad(): # stop tracking gradients for faster speed / lower memory\n",
    "                for X, y in dataloader:\n",
    "                    X, y = X.to(self.device), y.to(self.device) # move to device\n",
    "                    y_pred = self.neural_net(X) # forward prop\n",
    "                    loss_val += self.loss_fn(y_pred, y).item() # get error\n",
    "            loss_val /= len(dataloader)\n",
    "\n",
    "            print(f'Epoch: {epoch} | Train Loss: {loss_train} | Val Loss: {loss_val}')\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        self.neural_net.eval()\n",
    "        dataloader = DataLoader(dataset_train, batch_size=len(dataset))\n",
    "        X, y = next(iter(dataloader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.neural_net(X.to(self.device))\n",
    "            loss = self.loss_fn(y_pred, y.to(self.device))\n",
    "        return y.numpy(), y_pred.argmax(axis=1).cpu().numpy(), loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d85678-39ca-4496-9741-9cb921b2bacc",
   "metadata": {},
   "source": [
    "### A Linear Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd193ee4-0ac6-4397-ade6-2d29c67086a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(28*28, 10) # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ef417e5-321f-453a-bdbc-7a0189c33f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = SimpleLinearNet()\n",
    "model = GenericDeepLearningModel(\n",
    "    neural_net,\n",
    "    nn.CrossEntropyLoss(), # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "    torch.optim.SGD(neural_net.parameters(), lr=1e-3),\n",
    "    torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79e00792-588e-40ad-aaa9-a01d016a0e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 40.309708188188836 | Val Loss: 27.563773762188486\n",
      "Epoch: 1 | Train Loss: 18.879860138956538 | Val Loss: 19.302958558109015\n",
      "Epoch: 2 | Train Loss: 17.188329904478916 | Val Loss: 27.769250177117044\n",
      "Epoch: 3 | Train Loss: 17.090964487892517 | Val Loss: 23.671133071363496\n",
      "Epoch: 4 | Train Loss: 13.935152999581174 | Val Loss: 23.29815293146349\n",
      "Epoch: 5 | Train Loss: 14.806204048877067 | Val Loss: 19.170209091990742\n",
      "Epoch: 6 | Train Loss: 14.554472452148477 | Val Loss: 102.45612598939745\n",
      "Epoch: 7 | Train Loss: 16.065015227870738 | Val Loss: 23.26684105053131\n",
      "Epoch: 8 | Train Loss: 12.62532473054338 | Val Loss: 28.80706609162822\n",
      "Epoch: 9 | Train Loss: 12.247026356610846 | Val Loss: 32.01378312980188\n",
      "CPU times: total: 15.1 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41717131-26df-4223-9d7f-56f45b28dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.788513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79      1001\n",
      "           1       0.91      0.96      0.94      1127\n",
      "           2       0.94      0.54      0.69       991\n",
      "           3       0.45      0.95      0.61      1032\n",
      "           4       0.96      0.83      0.89       980\n",
      "           5       0.91      0.69      0.79       863\n",
      "           6       0.92      0.88      0.90      1014\n",
      "           7       0.98      0.79      0.88      1070\n",
      "           8       0.91      0.35      0.51       944\n",
      "           9       0.86      0.74      0.80       978\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.85      0.77      0.78     10000\n",
      "weighted avg       0.85      0.78      0.78     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y, y_pred, loss = model.evaluate(dataset_test)\n",
    "print(loss)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd6e41-6e12-4ed4-b6b3-42e707e86ab6",
   "metadata": {},
   "source": [
    "### A Nonlinear Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f21ae47a-c164-4889-86f4-4414e9642a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(28*28, 256) # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.layer2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d32a6f0d-8091-4258-b8aa-111c0921c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = NonLinearNet()\n",
    "model = GenericDeepLearningModel(\n",
    "    neural_net,\n",
    "    nn.CrossEntropyLoss(), # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "    torch.optim.SGD(neural_net.parameters(), lr=1e-3),\n",
    "    torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bbcb428-1a67-4bdc-83e0-e1a841134dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 9.012587685058726 | Val Loss: 2.6126150126948255\n",
      "Epoch: 1 | Train Loss: 1.8587109403724367 | Val Loss: 1.8618727823389745\n",
      "Epoch: 2 | Train Loss: 1.201519498561925 | Val Loss: 1.367805096914551\n",
      "Epoch: 3 | Train Loss: 0.9419910249974024 | Val Loss: 1.7320785754397987\n",
      "Epoch: 4 | Train Loss: 0.7969243927521908 | Val Loss: 1.2164581948474271\n",
      "Epoch: 5 | Train Loss: 0.7177809138802138 | Val Loss: 0.9669188527640566\n",
      "Epoch: 6 | Train Loss: 0.6241747729995466 | Val Loss: 0.8164463173677482\n",
      "Epoch: 7 | Train Loss: 0.5663915293370473 | Val Loss: 0.7931393837449012\n",
      "Epoch: 8 | Train Loss: 0.5356573009110511 | Val Loss: 0.8314514110671051\n",
      "Epoch: 9 | Train Loss: 0.510583781934482 | Val Loss: 0.6994510111885507\n",
      "CPU times: total: 14.5 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a5d627e-3414-4c99-b37a-48a6fb75191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6797344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95      1001\n",
      "           1       0.91      0.98      0.94      1127\n",
      "           2       0.91      0.85      0.88       991\n",
      "           3       0.76      0.93      0.84      1032\n",
      "           4       0.87      0.94      0.90       980\n",
      "           5       0.86      0.82      0.84       863\n",
      "           6       0.94      0.92      0.93      1014\n",
      "           7       0.92      0.87      0.90      1070\n",
      "           8       0.87      0.72      0.79       944\n",
      "           9       0.86      0.87      0.86       978\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.89      0.88      0.88     10000\n",
      "weighted avg       0.89      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y, y_pred, loss = model.evaluate(dataset_test)\n",
    "print(loss)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bfa8bb-cd41-42e2-b061-048686e82d6f",
   "metadata": {},
   "source": [
    "### Slightly More Complex Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "181c56b3-77d6-47da-93d9-6f08fba8756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(28*28, 256) # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.layer2 = nn.Linear(256, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.bn = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6cdeba19-afd3-41c5-a76d-b3c9ad7ac9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = ComplexNet()\n",
    "model = GenericDeepLearningModel(\n",
    "    neural_net,\n",
    "    nn.CrossEntropyLoss(), # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "    torch.optim.SGD(neural_net.parameters(), lr=1e-3),\n",
    "    torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e7f705c-2b41-4eee-bb57-ac22bba028a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 2.035435875679584 | Val Loss: 1.5648306749268635\n",
      "Epoch: 1 | Train Loss: 1.4361732855756233 | Val Loss: 1.1783731187711646\n",
      "Epoch: 2 | Train Loss: 1.1583283790882597 | Val Loss: 0.9707393722493511\n",
      "Epoch: 3 | Train Loss: 0.9952280993791337 | Val Loss: 0.8446643254650172\n",
      "Epoch: 4 | Train Loss: 0.8866599582611246 | Val Loss: 0.7582155691344601\n",
      "Epoch: 5 | Train Loss: 0.80497352588684 | Val Loss: 0.6920147496245818\n",
      "Epoch: 6 | Train Loss: 0.749968855304921 | Val Loss: 0.6410865177795577\n",
      "Epoch: 7 | Train Loss: 0.7023762816444357 | Val Loss: 0.5989109164139609\n",
      "Epoch: 8 | Train Loss: 0.6637834835242717 | Val Loss: 0.5643923942531858\n",
      "Epoch: 9 | Train Loss: 0.6339575144204688 | Val Loss: 0.5410071620618356\n",
      "CPU times: total: 16 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f454345a-6ae1-478e-b3da-0b35bf4ce883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5314726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94      1001\n",
      "           1       0.86      0.98      0.91      1127\n",
      "           2       0.90      0.84      0.87       991\n",
      "           3       0.85      0.83      0.84      1032\n",
      "           4       0.90      0.90      0.90       980\n",
      "           5       0.87      0.79      0.83       863\n",
      "           6       0.89      0.94      0.92      1014\n",
      "           7       0.90      0.90      0.90      1070\n",
      "           8       0.88      0.80      0.84       944\n",
      "           9       0.86      0.87      0.86       978\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y, y_pred, loss = model.evaluate(dataset_test)\n",
    "print(loss)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab44f91b-d405-460f-a89d-c9784a32c993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "def conv_dim(in_dim, kernel, stride=1, padding=0, dilation=1):\n",
    "    out_dim = int((in_dim + 2*padding - dilation*(kernel - 1) - 1)/stride + 1)\n",
    "    return out_dim\n",
    "\n",
    "dim = conv_dim(28, 3, stride=2)\n",
    "dim = conv_dim(dim, 3, stride=1)\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de3dd3-c05c-42c8-b64e-2ffb49b7a762",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "051d0895-9f1b-43e0-bc57-2affebb87937",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.add_channel_dimension()\n",
    "dataset_test.add_channel_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de916e78-320b-4b00-b40e-dee36e18f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential( # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "            nn.Conv2d(1, 16, 3, stride=2), # \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*11*11, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57c74492-8cf0-422c-a0aa-7194ad5cc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = CNN()\n",
    "model = GenericDeepLearningModel(\n",
    "    neural_net,\n",
    "    nn.CrossEntropyLoss(), # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "    torch.optim.SGD(neural_net.parameters(), lr=1e-3),\n",
    "    torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "040c2964-249c-4999-bb57-6801229c4363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 1.3475130422318236 | Val Loss: 0.863038068418818\n",
      "Epoch: 1 | Train Loss: 0.6998353286626491 | Val Loss: 0.5928574696437382\n",
      "Epoch: 2 | Train Loss: 0.5260738112190937 | Val Loss: 0.4832980863448145\n",
      "Epoch: 3 | Train Loss: 0.4411310204800139 | Val Loss: 0.4183460044057003\n",
      "Epoch: 4 | Train Loss: 0.38961710812563594 | Val Loss: 0.3761330754565659\n",
      "Epoch: 5 | Train Loss: 0.352353443332175 | Val Loss: 0.3460389236365554\n",
      "Epoch: 6 | Train Loss: 0.32477523378552275 | Val Loss: 0.3242693642563403\n",
      "Epoch: 7 | Train Loss: 0.3023293821260016 | Val Loss: 0.30434898843071356\n",
      "Epoch: 8 | Train Loss: 0.28412980110721386 | Val Loss: 0.2880167916758673\n",
      "Epoch: 9 | Train Loss: 0.2691962500201895 | Val Loss: 0.27557560721281243\n",
      "CPU times: total: 20.7 s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da0f0cfa-08f0-4740-ab78-a54c94be9f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27075937\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97      1001\n",
      "           1       0.95      0.97      0.96      1127\n",
      "           2       0.92      0.92      0.92       991\n",
      "           3       0.93      0.90      0.91      1032\n",
      "           4       0.94      0.95      0.95       980\n",
      "           5       0.91      0.94      0.92       863\n",
      "           6       0.95      0.96      0.95      1014\n",
      "           7       0.95      0.92      0.93      1070\n",
      "           8       0.92      0.88      0.90       944\n",
      "           9       0.90      0.91      0.90       978\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y, y_pred, loss = model.evaluate(dataset_test)\n",
    "print(loss)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfe034-2e0b-43bc-899b-8ddca02a551d",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06d8826c-7816-4a53-b296-8dbb7c13c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "neural_net = resnet50()\n",
    "neural_net._modules['conv1'] = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model = GenericDeepLearningModel(\n",
    "    neural_net,\n",
    "    nn.CrossEntropyLoss(), # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "    torch.optim.SGD(neural_net.parameters(), lr=1e-3),\n",
    "    torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6304b17-e49a-4448-97ce-ad330365dd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 2.789503958631069 | Val Loss: 2.16271891459219\n",
      "Epoch: 1 | Train Loss: 1.977960412172561 | Val Loss: 1.7490277604253561\n",
      "Epoch: 2 | Train Loss: 1.5177641029053546 | Val Loss: 1.2651890675142121\n",
      "Epoch: 3 | Train Loss: 1.0693497879708067 | Val Loss: 0.8981671008918839\n",
      "Epoch: 4 | Train Loss: 0.7625468489020428 | Val Loss: 0.6419392328685535\n",
      "Epoch: 5 | Train Loss: 0.5773817039233573 | Val Loss: 0.5058422752979722\n",
      "Epoch: 6 | Train Loss: 0.4585145971083895 | Val Loss: 0.418229294658851\n",
      "Epoch: 7 | Train Loss: 0.36243749069089587 | Val Loss: 0.3608058339306541\n",
      "Epoch: 8 | Train Loss: 0.29723355388070677 | Val Loss: 0.3110300861040889\n",
      "Epoch: 9 | Train Loss: 0.2468104509914175 | Val Loss: 0.2733064153646705\n",
      "CPU times: total: 2min 41s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "827e46a8-c46d-4fcb-96f2-674fc4124d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27195197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1001\n",
      "           1       0.96      0.97      0.96      1127\n",
      "           2       0.91      0.90      0.91       991\n",
      "           3       0.90      0.88      0.89      1032\n",
      "           4       0.91      0.92      0.91       980\n",
      "           5       0.85      0.93      0.89       863\n",
      "           6       0.95      0.94      0.95      1014\n",
      "           7       0.92      0.90      0.91      1070\n",
      "           8       0.90      0.84      0.87       944\n",
      "           9       0.89      0.90      0.89       978\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y, y_pred, loss = model.evaluate(dataset_test)\n",
    "print(loss)\n",
    "print(classification_report(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
